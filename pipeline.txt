The current pipeline, this file is just different commands someone can execute.

The data is located in these files, which are included in the git repository.
/data2019/ -> folder with extracted corpus subsets and training data
/features2019/ -> folder with feature vectors in libsvm format
/model2019/ -> folder with pickled X_trans, y_trans, model

execute commands in the framework folder
python3 extract_open_corpus.py #given open corpus file, extract data to TREC format (not usuable atm)
python3 loader.py #fill config file
python3 data_combining.py #print data of specific article

These steps are to replicate the trained model, which is currently stored in /model2019/
Execute them in the /framework/ folder
It assumed that the following file exist:
data2019/fair-TREC-training-sample.json: training file (to know which documents to extract)

Step 1, create subset for corpus:
python3 create_corpus_subset.py -t ../data2019/fair-TREC-training-sample.json -o ../data2019/corpus-subset-for-queries.jsonl

Step 2, extract corpus:
python3 extract_open_corpus.py -c ../data2019/corpus-subset-for-queries.jsonl -s ../data2019/

Step 3, update config file

Step 4, extract features and store it in libsvm format:
python3 feature_extraction.py -f ../data2019/fair-TREC-training-sample.json -o ../features2019
python3 feature_extraction.py -f ../data2020/TREC-Fair-Ranking-training-sample.json -o ../features2020

Step 5, train using
python3 svm_training.py -f ../features2019/libsvm.txt -p ../model2019 -i 1000000000

Step 6, score model
python3 svm_training.py --score -p ../model2019/
python3 model_ranking.py -m ../model2019/model -f ../features2019/libsvm.txt -g ../data2019/fair-TREC-sample-author-groups.csv -l ../features2019/linker.txts
