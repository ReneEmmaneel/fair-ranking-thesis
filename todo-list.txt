Timeline:

Fri 18 dec - First draft of thesis
Fri 15 Jan - Go/No-go version of thesis
Fri 22 Jan - Final version of thesis
Week of 25-29 Jan - Presentation

General todo-list:

Things to complete in the sort term:

[X] Add fairness function for ranking of model

[]  Continue on thesis about things already implemented
[X] Start writing about fair ranking in thesis + continue reading papers
[]  Work on describing the framework

[X] Make plan about needed functionality for fair ranking
[]  Create adjust-for-exposure algorithm (not very useful, but should be easy and makes it better to write about it in the thesis)
[]  (start to) Implement Fair-PGRank as described by https://arxiv.org/pdf/2005.14713.pdf https://github.com/ashudeep/Fair-PGRank

List of things that are completed:

[X] Make loader, which takes in data files as arguments and put the file paths into config files
[X] Make function for processing the files to datatables using pandas

[X] Learn about the "Semantic Scolar (S2) Open Corpus" and compare it to the TREC2020 fair ranking track format
[X] Create a function for using the API provided by the open corpus to download the needed Open Corpus document data
[X] Create a function to download all the data and turn the needed data for 2019 into the corpus subset format like in 2020
[X] Make a function for turning S2 Open Corpus data into TREC format (in other words, given the corpus.jsonl, generate authors.csv, papers.csv, paper_authors.csv)
[X] Extract the data
[X] Add the data to git (using git-lfs for big files)

[X] Create function to combine data from these different files given a document id
[X] Learn about Support Vector Machines
[X] Learn about Ranking SVM

[X] Make start of python file using sklearn to use tf-idf for feature extraction
[X] Make python file for creating and saving tf-idf data for the entire corpus
[X] Make python file to create a framework for extracting features given query + document id (Using previous file)
[X] Make TF, IDF and TF_IDF features
[X] Make BM25 features
[X] Make lmir features (abs, jm, dir)
[X] Other simple features regarding the metadata of the document using the sementic scolar open corpus data
[X] Analyse previous work for determining which (types of) features should be extracted

This feature framework should be as robust as possible because there are many possible features,
potentially dozens will be used. It is hard to say what is needed for the final product,
so for this step just a simple framework using a few sklearn functions is enough.

[X] Make a function to turn (Query, document id) pairs into feature vectors
[X] Make a function to turn training data (see TREC-Fair-Ranking-training-sample.json) into SVM
[X] Use the training set to create a (framework for creating a) simple Ranking SVM model
[X] Make a file for creating function to measure on how well the model ranks
[X] Add DCG function

Things maybe to be implemented in the future:

[]  Create function to extract data for authors, using the semantic scolar open corpus
[]  Create different groups, as defined by the 2019 fair ranking track
