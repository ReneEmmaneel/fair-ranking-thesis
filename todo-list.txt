General todo-list:

This list represents a plan on the short (1 to 2 weeks) term:

[x] Make loader, which takes in data files as arguments and put the file paths into config files
[x] Make function for processing the files to datatables using pandas

[X] Learn about the "Semantic Scolar (S2) Open Corpus" and compare it to the TREC2020 fair ranking track format
[X] Extract articles' meta data from "Semantic Scholar (S2) Open Corpus" and save the data in the same format as TREC2020 fair ranking track.
  Sort of done, the Open Corpus data given by TREC is used
[]  Make a function for turning S2 Open Corpus data into TREC format (in other words, given the corpus.jsonl, generate authors.csv, papers.csv, paper_authors.csv)

[X] Create function to combine data from these different files given a document id
[X] Learn about Support Vector Machines
[X] Learn about Ranking SVM

[X] Make start of python file using sklearn to use tf-idf for feature extraction
[X] Make python file for creating and saving tf-idf data for the entire corpus
[X] Make python file to create a framework for extracting features given query + document id (Using previous file)
[X] Make TF, IDF and TF_IDF features
[X] Make BM25 features
[/] Other simple features regarding the metadata of the document using the sementic scolar open corpus data
[]  Analyse previous work for determining which (types of) features should be extracted

This feature framework should be as robust as possible because there are many possible features,
potentially dozens will be used. It is hard to say what is needed for the final product,
so for this step just a simple framework using a few sklearn functions is enough.

[X] Make a function to turn (Query, document id) pairs into feature vectors
[X] Make a function to turn training data (see TREC-Fair-Ranking-training-sample.json) into SVM
[]  Use the training set to create a (framework for creating a) simple Ranking SVM model
